{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hitha-varganti/matching-game/blob/main/Copy_of_Student_ObjectDetection_Section3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wJjl_umy-DX"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8SzjAnb5ubi"
      },
      "source": [
        "# Milestone 1. What is YOLO?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG9wqmq4Qd2_"
      },
      "source": [
        "\n",
        "The “You Only Look Once,” or YOLO, family of models are a series of end-to-end deep learning models designed for fast object detection, developed by Joseph Redmon, et al. and first proposed in the 2015 paper titled “[You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640).” The model has been updated since then. Today, we'll focus on YOLOv3, which is described in this very interesting [technical report](https://pjreddie.com/media/files/papers/YOLOv3.pdf). Today, we'll walk through the basic idea of the algorithm. If you'd like to know more details about it, definitely check out the papers!\n",
        "\n",
        "The approach involves a single deep convolutional neural network (DarkNet which is based on the VGG model we used before) that splits the input into a grid of cells and each cell directly predicts a bounding box and object classification. The result is a large number of candidate bounding boxes that are consolidated into a final prediction by a post-processing step.\n",
        "\n",
        "For example, an image may be divided into a 7×7 grid and each cell in the grid may predict 2 bounding boxes, resulting in 98 proposed bounding box predictions. The class probabilities map and the bounding boxes with confidences are then combined into a final set of bounding boxes and class labels. The image taken from the paper below summarizes the two outputs of the model.\n",
        "\n",
        "In summary, to make object detection on one input image, the first step is a forward pass of the DarkNet; the second step is the post-processing on the DarkNet output to get the final bounding boxes prediction.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1-IuBrnrTZPOb4zZXGLl6Kn4OKsX9NnXR)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP3N7y0pPJb-"
      },
      "source": [
        "# Milestone 2. How does YOLO work?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc-OEZJeyEms"
      },
      "source": [
        "Before we proceed to build the YOLO model, let's first define the **anchor boxes**, which are several pre-defined bounding boxes with useful shapes and sizes that are tailored based on the object shapes in the training dataset.\n",
        "\n",
        "There are 9 anchor boxes in total. As we'll talk about later, the detection is performed on 3 scales. Therefore, the anchor boxes are divided into 3 groups, each corresponding to 1 scale.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaEX1KApyaHV"
      },
      "source": [
        "anchors = [[[116,90], [156,198], [373,326]], [[30,61], [62,45], [59,119]], [[10,13], [16,30], [33,23]]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpK7kGhVycHY"
      },
      "source": [
        "\n",
        "The 9 anchor boxes are plotted below. As you may discover, they can cover a variety of shapes and sizes.\n",
        "\n",
        "<img src=\"http://www.programmersought.com/images/401/891354390c3aab3f1ab1fd0db3110bf9.png\" width=\"400\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v7dgrxMggcP"
      },
      "source": [
        "Now, let's load the image that we'll apply object detection on. To load the image, we'll use the `Image` module in the package `PIL`, which is commonly used  for image processing. The image is saved as a `PIL image` in the variable `image_pil`. We can get the width and the height of the image by accessing the `size` attribute of the image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kna4URqwgxR3"
      },
      "source": [
        "from PIL import Image\n",
        "from matplotlib import  pyplot as plt\n",
        "\n",
        "image_path = '/content/data/image.jpg'\n",
        "\n",
        "image_pil = Image.open(image_path)\n",
        "image_w, image_h = image_pil.size\n",
        "print(\"The type of the saved image is {}\".format(type(image_pil)))\n",
        "plt.imshow(image_pil)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PangGSekeos"
      },
      "source": [
        "### Exercise (Coding) | Image Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqI9ik44jiv3"
      },
      "source": [
        "The input size of DarkNet is `(416, 416)`, so we need to preprocess our image into the required size by resizing our image, keeping the aspect ratio consistent, and padding the left out areas with the grey color, which is `(128,128,128)` in RGB. We have implemented the preprocessing for you in the `preprocess_input(image, net_h, net_w)` function, which takes the orininal image, the target height and width `net_h, net_w` as input and returns the new image in the required size.\n",
        "\n",
        "In the chunk below, do the preprocessing by yourself! Plot the new image to check your result\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivoUD7DCkOyj"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "# new_image =\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emIW2zh9l1pa"
      },
      "source": [
        "#@title Run this to check the new image { display-mode: \"form\" }\n",
        "plt.imshow(new_image[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taZ8Ux4bmIxL"
      },
      "source": [
        "### Exercise (Discussion) | DarkNet Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUkj4NHMcEUj"
      },
      "source": [
        "The main part of the YOLO algorithm is the DarkNet model, which is basiclly a Convolutional Neural Network, with some special designs, like upsampling layers and detection layers.\n",
        "\n",
        "Here is how the architecture of DarkNet looks like:\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/2000/1*d4Eg17IVJ0L41e7CTWLLSg.png\" width=\"1000\"/>\n",
        "\n",
        "**The residual blocks** in the picture contain layers that are similar to the CNN models we built before, eg. convolutional layers `Conv2D`, max pooling layers `MaxPooling2D`, activation layers `Activation('relu')`. The network just stacks a lot more layers than the model we built before.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjJET2R__7tm"
      },
      "source": [
        "**How to make detections at 3 different scales?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WypkCU7Fm5Ev"
      },
      "source": [
        "Besides the stuff that we are familiar with, the most salient feature of YOLOv3 DarkNet is that it makes detections at three different scales, which are precisely given by downsampling the dimensions of the input image by 32, 16 and 8 respectively.\n",
        "\n",
        "The first detection is made by the 82nd layer. For the first 81 layers, the image is down sampled by the network, such that the 81st layer has a stride of 32. If we have an image of 416 x 416, the resultant feature map would be of size 13 x 13.\n",
        "\n",
        "The feature map size at layer 94 and 106 grows bigger because of the upsampling layers. The feature maps are upsampled by 2x to dimensions of 26 x 26 and 52 x 52 respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik1TiYtjAgv8"
      },
      "source": [
        "**What exactly are the DarkNet outputs?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alArj1hP_3SG"
      },
      "source": [
        "The eventual detection output is generated by applying detection kernels on feature maps at the three different places in the network.\n",
        "\n",
        "For each grid cell, we'll consider several possible bounding boxes that are centered at the given grid cell. Then for each considered bounding box, the model predicts t<sub>x</sub>, t<sub>y</sub>, t<sub>w</sub>, t<sub>h</sub>, an objectness score, and class scores.\n",
        "- t<sub>x</sub>, t<sub>y</sub>, t<sub>w</sub>, t<sub>h</sub> are related to predicting the exact position and shape of the considered bounding box.\n",
        "- The objectness score is the model's prediction about how likely the considered bounding box has a complete object inside it.\n",
        "- Class scores are the predicted probability over all the object classes.\n",
        "\n",
        "Therefore, the shape of the detection kernel is 1 x 1 x (B x (4 + 1 + C)). Here, 1 x 1 means the kernel only looks at one grid cell at one time. B is the number of bounding boxes a cell on the feature map can predict, \"4\" is for the 4 bounding box attributes (t<sub>x</sub>, t<sub>y</sub>, t<sub>w</sub>, t<sub>h</sub>) and \"1\" for the object confidence. C is the number of object classes.\n",
        "\n",
        "The model will consider bounding boxes based on the 3 anchor boxes defined before, so B = 3. As YOLO is trained on COCO (a large-scale object detection dataset), which contains 80 object catogories, C = 80. Therefore, the kernel size is 1 x 1 x 255. The feature map produced by this kernel has identical height and width of the previous feature map, and has detection attributes along the depth as described above.\n",
        "\n",
        "The following picture illustrates how this works.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1200/0*3A8U0Hm5IKmRa6hu.png\" width=\"500\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie0s7V0b0b_h"
      },
      "source": [
        "### Exercise (Coding and Discussion) | Forward Pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET3JrlF-0izF"
      },
      "source": [
        "Now, let's load a fully trained DarkNet model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-ALek3J0Zk4"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load model\n",
        "darknet = tf.keras.models.load_model(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1InpAytA1j8i"
      },
      "source": [
        "Just as how we got the classification predictions from the perceptron, CNN, and VGG models, call the `model.predict(input_data)` function to do a forward pass on our preprocessed image `new_image`!\n",
        "\n",
        "After you get the output, check the structure of the output and discuss what the dimensions mean with you classmates!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMcecVTq1X1f"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "# yolo_outputs =\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqu3tVsC3paa"
      },
      "source": [
        "Answer the following questions:\n",
        "\n",
        "\n",
        "*   How many elements are there in the `yolo_outputs`? Why?\n",
        "*   What does each dimension of the `yolo_outputs[0]` mean?\n",
        "*   Why the last dimension is 255?\n",
        "\n",
        "If you are clear about the questions above, now you can definitely explain how the DarkNet works to your classmates! (At each detection scale, ... For each grid cell, ... For each bounding boxes, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUjzOeRaPSaE"
      },
      "source": [
        "# Milestone 3. Bounding Box Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-O9CWNg5pcS"
      },
      "source": [
        "We now have DarkNet's detection predictions for all the possible bounding boxes centered at each grid cell position, but to get the final detection results, which are the bounding boxes that the model is confident of, we need to apply a threshold to filter the results.\n",
        "\n",
        "Besides, as you can imagine, there might be multiple bounding boxes that are detecting the same object. We need to remove the overlapping bounding boxes and only leave the best ones.\n",
        "\n",
        "Here are some post-processing steps:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugs_ntcEOK8q"
      },
      "source": [
        "\n",
        "\n",
        "*   `decode_netout(yolo_outputs, obj_thresh, anchors, image_h, image_w, net_h, net_w)` takes the DarkNet output feature maps `yolo_outputs` as input, and returns all the predicted bounding boxes that have a higher objectness than the objectness threshold `obj_thresh`\n",
        "*   `do_nms(boxes, nms_thresh, obj_thresh)` means Non-Maximal Suppression, which a commonly used post-processing step for object detection. It  removes  all the bounding boxes that have a big (higher overlap than the `nms_thresh`) overlap with other better bounding boxes.\n",
        "*   `draw_boxes(image_pil, boxes, labels)` draws the final bounding boxes on the input image and return the detection image as a `PIL image`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHvF90RH5fXX"
      },
      "source": [
        "### Exercise (Coding) | Post-processing for bounding box prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjxtyuXWSbvh"
      },
      "source": [
        "First, let's define the thresholds mentioned above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDQ-zqD1Sh-t"
      },
      "source": [
        "obj_thresh = 0.4\n",
        "nms_thresh = 0.45"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE9FP_-5R3Km"
      },
      "source": [
        "Make use of the functions above to get our final detection bounding boxes and plot the result you get!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YNBAFy2CV-P"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC2EEvIwgvZT"
      },
      "source": [
        "### Exercise (Coding) | Non-Maximal Suppression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIMUP-LqhpGF"
      },
      "source": [
        "Good job! Are you curious about what each post-processing step is doing? You can explore this by yourself!\n",
        "\n",
        "As a hint, you can...\n",
        "\n",
        "*   Check the number of boxes after each step\n",
        "*   Call the `draw_boxes(image_pil, boxes, labels)` function to visualize the bounding boxes after each step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xNVUA8RiaFE"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqEuORB1jzJm"
      },
      "source": [
        "### Exercise (Coding) | Image Detection Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePIQrHdmkNlO"
      },
      "source": [
        "Our final goal is to detect objects in a video, which contains multiple frames (images). For better reusability and modularity, let's wrap all the code we wrote before in a function called `detect_image`, which takes the raw `PIL image` (without preprocessing) and other parameters as input, and returns the `PIl image` with detected bounding boxes and labels. Complete this function by yourself and test it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVL7K8pLVa5l"
      },
      "source": [
        "def detect_image(image_pil, obj_thresh = 0.4, nms_thresh = 0.45, darknet=darknet, net_h=416, net_w=416, anchors=anchors, labels=labels):\n",
        "  ### YOUR CODE HERE\n",
        "  pass\n",
        "  ### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK2Pm3pRmPCu"
      },
      "source": [
        "#@title Run this to check your function definition { display-mode: \"form\" }\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(detect_image(image_pil))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPn2dkmWn7Fk"
      },
      "source": [
        "### Exercise (Discussion) | Thresholds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlU20D9aoBSs"
      },
      "source": [
        "Up till now, We used default values for the 2 thresholds, `objectness threshold` and `nms_threshold`. Do you understand what these 2 thresholds control? Make use of the `detect_image`function we defined above, try different values for the 2 thresholds in the range of 0-1 and see the changes in the results. Then discuss this with your classmates!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q041DhjwP7wh"
      },
      "source": [
        "# Milestone 4. Detection on Videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsNm7ivW0fwl"
      },
      "source": [
        "A video is just a sequence of frames (images). Therefore, once we can use YOLO to detect objects on images, it's easy to extend this to videos. To deal with videos, we'll use the OpenCV package, which is called `cv2` in Python. If you are interested to know more, here is a [tutorial](https://docs.opencv.org/4.5.2/d0/de3/tutorial_py_intro.html).\n",
        "\n",
        "The code below will open one video, create a new video file, read the input video frame-by-frame, and write each frame into the new video.\n",
        "\n",
        "Now modify the code by yourself to get the object detection result on the input video!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHsORdfL3Weq"
      },
      "source": [
        "Remember that the image input for the `detect_image` function is a `PIL image`, but here we are loading the input video using `OpenCV`. These 2 image formats are different, so we need to convert `OpenCV` to `PIL` for detection, and convert back to write the frame into the new video.\n",
        "\n",
        "The conversion can be done as follows\n",
        "```\n",
        "# OpenCV -> PIL\n",
        "image_pil = Image.fromarray(cv2.cvtColor(image_cv2, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# PIL -> OpenCV\n",
        "image_cv2 = cv2.cvtColor(np.asarray(image_pil), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdCIcyOP7Mfj"
      },
      "source": [
        "import cv2\n",
        "\n",
        "def detect_video(video_path, output_path, obj_thresh = 0.4, nms_thresh = 0.45, darknet=darknet, net_h=416, net_w=416, anchors=anchors, labels=labels):\n",
        "    vid = cv2.VideoCapture(video_path)\n",
        "    if not vid.isOpened():\n",
        "        raise IOError(\"Couldn't open webcam or video\")\n",
        "    video_FourCC    = int(vid.get(cv2.CAP_PROP_FOURCC))\n",
        "    video_FourCC = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    video_fps       = vid.get(cv2.CAP_PROP_FPS)\n",
        "    video_size      = (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "                        int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "\n",
        "    out = cv2.VideoWriter(output_path, video_FourCC, video_fps, video_size)\n",
        "\n",
        "    num_frame = 0\n",
        "    while vid.isOpened():\n",
        "      ret, frame = vid.read()\n",
        "      num_frame += 1\n",
        "      print(\"=== Frame {} ===\".format(num_frame))\n",
        "      if ret:\n",
        "          ### YOUR CODE HERE\n",
        "          new_frame = frame\n",
        "\n",
        "          ### END CODE\n",
        "          out.write(new_frame)\n",
        "      else:\n",
        "          break\n",
        "    vid.release()\n",
        "    out.release()\n",
        "    print(\"New video saved!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEHFzqQn5R5h"
      },
      "source": [
        "Now test your code! You can check the videos in the FILES on the left"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-fRC2TW8Qkn"
      },
      "source": [
        "video_path = '/content/data/video1.mp4'\n",
        "output_path = '/content/data/video1_detected.mp4'\n",
        "detect_video(video_path, output_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}